# app.py (Versi贸n "Todo en Uno")

# --- Importaciones Esenciales ---
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModel
import matplotlib.pyplot as plt
import seaborn as sns

# --- Configuraci贸n de la P谩gina ---
st.set_page_config(
    page_title="Visor de Atenci贸n",
    page_icon="",
    layout="wide"
)

# --- Carga del Modelo (con Cach茅) ---
@st.cache_resource
def load_model():
    """Carga el modelo y el tokenizer de Hugging Face."""
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    model = AutoModel.from_pretrained('bert-base-uncased', output_attentions=True)
    return tokenizer, model

# --- T铆tulo Principal ---
st.title(" Visor del Mecanismo de Atenci贸n y Gu铆a")

# --- Creaci贸n de las Pesta帽as ---
# st.tabs crea un contenedor con pesta帽as. Cada bloque 'with' corresponde a una pesta帽a.
tab1, tab2 = st.tabs(["Visor de Atenci贸n", " Gu铆a de Ejemplos"])


# --- Contenido de la Pesta帽a 1: Visor de Atenci贸n ---
with tab1:
    st.header("Visualizador Interactivo de Atenci贸n")
    st.write(
        "Esta aplicaci贸n te permite ver c贸mo funciona el mecanismo de 'atenci贸n' dentro de un modelo Transformer (BERT). "
        "Introduce una frase y observa qu茅 palabras le 'prestan atenci贸n' a otras para construir su significado contextual."
    )

    with st.spinner("Cargando modelo pre-entrenado..."):
        tokenizer, model = load_model()

    st.subheader("Introduce tu frase")
    user_input = st.text_area(
        "Texto a analizar:",
        "The quick brown fox jumps over the lazy dog.",
        height=100,
        key="main_input" # Se a帽ade una 'key' para diferenciarlo de otros text_area
    )

    col1, col2 = st.columns(2)
    layer_to_visualize = col1.slider("Capa de Atenci贸n a Visualizar", 0, 11, 6)
    head_to_visualize = col2.slider("Cabeza de Atenci贸n a Visualizar", 0, 11, 0)

    if st.button("Visualizar Atenci贸n"):
        if user_input:
            inputs = tokenizer(user_input, return_tensors='pt', add_special_tokens=True)
            token_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

            with torch.no_grad():
                outputs = model(**inputs)

            attention_matrix = outputs.attentions[layer_to_visualize][0, head_to_visualize].numpy()

            st.subheader(f"Mapa de Calor de Atenci贸n (Capa {layer_to_visualize}, Cabeza {head_to_visualize})")
            st.write(
                "Este mapa muestra la puntuaci贸n de atenci贸n de cada palabra (eje Y) hacia cada otra palabra (eje X). "
                "Un color m谩s brillante significa una puntuaci贸n de atenci贸n m谩s alta."
            )

            fig, ax = plt.subplots(figsize=(10, 8))
            sns.heatmap(attention_matrix, xticklabels=token_list, yticklabels=token_list, cmap='viridis', ax=ax)
            plt.xticks(rotation=45)
            ax.set_xlabel("Palabra a la que se 'presta atenci贸n' (Key)")
            ax.set_ylabel("Palabra que 'presta atenci贸n' (Query)")

            st.pyplot(fig)
        else:
            st.warning("Por favor, introduce una frase para analizar.")


# --- Contenido de la Pesta帽a 2: Gu铆a de Ejemplos ---
with tab2:
    st.header("Gu铆a de Ejemplos para el Visor de Atenci贸n")
    st.markdown(
        """
        Esta gu铆a proporciona varios ejemplos para probar en el **Visor de Atenci贸n**. 
        Cada ejemplo est谩 dise帽ado para revelar un aspecto diferente de c贸mo el mecanismo 
        de atenci贸n de un Transformer interpreta el lenguaje.
        """
    )
    st.divider()

    st.subheader("1. Resoluci贸n de Pronombres", anchor=False)
    st.markdown("Muestra si el modelo vincula un pronombre (como 'it') al sustantivo al que se refiere.")
    st.code("The robot picked up the ball because it was heavy.", language="text")
    st.info("**Qu茅 esperar:** En la fila de `it`, busca un color brillante en la columna de `ball`.")

    st.subheader("2. Relaci贸n Sujeto-Verbo-Objeto", anchor=False)
    st.markdown("Revela c贸mo la acci贸n (verbo) se conecta con quien la realiza (sujeto) y quien la recibe (objeto).")
    st.code("The programmer wrote the code.", language="text")
    st.info("**Qu茅 esperar:** En la fila de `wrote`, busca colores brillantes en las columnas de `programmer` y `code`.")
    
    st.subheader("3. Dependencias a Larga Distancia", anchor=False)
    st.markdown("Prueba la capacidad del Transformer para conectar palabras que est谩n muy separadas.")
    st.code("The dog that chased the cat across the yard finally took a nap.", language="text")
    st.info("**Qu茅 esperar:** En la fila de `nap` (o `took`), busca un color brillante en la columna de `dog`.")
