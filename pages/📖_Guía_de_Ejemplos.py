# pages/_Gu铆a_de_Ejemplos.py

import streamlit as st

# --- Configuraci贸n de la P谩gina ---
st.set_page_config(
    page_title="Gu铆a de Ejemplos",
    page_icon="",
    layout="wide"
)

# --- T铆tulo y Descripci贸n ---
st.title(" Gu铆a de Ejemplos para el Visor de Atenci贸n")
st.markdown(
    """
    Esta gu铆a proporciona varios ejemplos para probar en el **Visor de Atenci贸n**. 
    Cada ejemplo est谩 dise帽ado para revelar un aspecto diferente de c贸mo el mecanismo 
    de atenci贸n de un Transformer interpreta el lenguaje.
    """
)

# --- Separador ---
st.divider()

# --- Ejemplo 1: Resoluci贸n de Pronombres ---
st.subheader("1. Resoluci贸n de Pronombres", anchor=False)
st.markdown(
    """
    Esta prueba muestra si el modelo puede vincular correctamente un pronombre (como 'it', 'he', 'she') 
    al sustantivo al que se refiere.
    """
)
st.code("The robot picked up the ball because it was heavy.", language="text")
st.info(
    """
    **Qu茅 esperar:** Busca la fila del token **`it`**. Deber铆as ver un color muy brillante 
    en la columna del token **`ball`**. Esto demuestra que el modelo entiende que "it" se refiere 
    a "the ball", y no a "the robot".
    """
)

# --- Ejemplo 2: Relaci贸n Sujeto-Verbo-Objeto ---
st.subheader("2. Relaci贸n Sujeto-Verbo-Objeto", anchor=False)
st.markdown(
    """
    Esta prueba revela c贸mo la acci贸n principal (el verbo) se conecta con quien la realiza (sujeto) 
    y quien la recibe (objeto).
    """
)
st.code("The programmer wrote the code.", language="text")
st.info(
    """
    **Qu茅 esperar:** En la fila del verbo **`wrote`**, deber铆as ver puntuaciones de atenci贸n altas 
    (colores brillantes) tanto para **`programmer`** (el sujeto) como para **`code`** (el objeto). 
    Esto muestra c贸mo la atenci贸n mapea la estructura gramatical.
    """
)

# --- Ejemplo 3: Dependencias a Larga Distancia ---
st.subheader("3. Dependencias a Larga Distancia", anchor=False)
st.markdown(
    """
    Los Transformers destacan por su capacidad para conectar palabras que est谩n muy separadas en una oraci贸n. 
    Este ejemplo prueba esa capacidad.
    """
)
st.code("The dog that chased the cat across the yard finally took a nap.", language="text")
st.info(
    """
    **Qu茅 esperar:** Busca la fila del verbo final, **`nap`** (o `took`). Deber铆a mostrar una puntuaci贸n 
    de atenci贸n sorprendentemente alta para el sujeto principal, **`dog`**, a pesar de la distancia.
    """
)

# --- Ejemplo 4: Resoluci贸n de Ambig眉edad ---
st.subheader("4. Resoluci贸n de Ambig眉edad", anchor=False)
st.markdown(
    """
    Esta prueba muestra c贸mo el contexto cambia el significado de una palabra y, por lo tanto, 
    sus patrones de atenci贸n.
    """
)
st.code("The bank of the river is steep.\nHe deposited money in the bank.", language="text")
st.info(
    """
    **Qu茅 esperar:** En la primera frase, **`bank`** prestar谩 fuerte atenci贸n a **`river`**. 
    En la segunda, **`bank`** prestar谩 atenci贸n a **`money`** y **`deposited`**. Esto prueba que la atenci贸n 
    es din谩mica y depende del contexto.
    """
)

# --- Ejemplo 5: Puntuaci贸n y Tokens Especiales ---
st.subheader("5. Puntuaci贸n y Tokens Especiales", anchor=False)
st.markdown(
    """
    Esto te ayuda a ver c贸mo el modelo trata los tokens que no son palabras, como la puntuaci贸n 
    o los tokens de control que usa internamente.
    """
)
st.code("Apples are sweet.", language="text")
st.info(
    """
    **Qu茅 esperar:** El punto final (`.`) probablemente prestar谩 atenci贸n a la 煤ltima palabra, **`sweet`**. 
    Tambi茅n ver谩s tokens especiales como `[CLS]` y `[SEP]`. El token `[CLS]` a menudo agrega 
    el significado de toda la oraci贸n, por lo que podr铆a prestar atenci贸n a muchos tokens a la vez.
    """
)
